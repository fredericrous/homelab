version: '3'

vars:
  ROOT_DIR: '../..'
  TERRAFORM_DIR: '{{.ROOT_DIR}}/infrastructure/homelab'
  KUBECONFIG: '{{.ROOT_DIR}}/kubeconfig'
  TALOSCONFIG: '{{.ROOT_DIR}}/talosconfig'

env:
  KUBECONFIG: '{{.KUBECONFIG}}'
  SHELL: /bin/bash

tasks:
  # Helper task to check prerequisites
  check-prereq:
    desc: Check prerequisites
    silent: true
    cmds:
      - |
        if ! command -v yq &> /dev/null; then
          echo "ERROR: yq is required but not installed. Install with: brew install yq"
          exit 1
        fi
      - |
        if [ ! -f "{{.ROOT_DIR}}/.env" ]; then
          echo "ERROR: .env file not found. Copy .env.example and update with your values"
          exit 1
        fi

  up:
    desc: Create cluster infrastructure (VMs + Talos + CNI)
    deps: [check-prereq]
    cmds:
      - task: init
      - task: provision
      - task: configure
      - task: kubeconfig
      - task: install-cilium
      - task: validate-cluster

  sync-env-secrets:
    desc: "Sync environment variables to cluster-vars secret"
    aliases: [load-env]
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    preconditions:
      - sh: kubectl get nodes
        msg: "Cluster must be accessible"
    cmds:
      - |
        echo "üîê Creating/updating cluster variables secret from .env..."
        ./bootstrap-cluster-vars.sh

  create-vault-transit-secret:
    desc: "Create vault-transit-token secret for Vault auto-unseal"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    preconditions:
      - sh: kubectl get nodes
        msg: "Cluster must be accessible"
    cmds:
      - |
        echo "üîê Creating vault-transit-token secret..."
        ./bootstrap-vault-transit-secret.sh

  install:
    desc: Full installation (cluster + GitOps + apps)
    deps: [check-prereq]
    cmds:
      - task: bootstrap
      - task: verify

  init:
    desc: Initialize Terraform
    dir: '{{.TERRAFORM_DIR}}'
    deps: [check-prereq]
    preconditions:
      - sh: test -f terraform.tfvars
      - msg: "terraform.tfvars not found. Copy terraform.tfvars.example and update with your values"
    cmds:
      - terraform init -upgrade

  provision:
    desc: "Provision VMs on Proxmox"
    dir: '{{.TERRAFORM_DIR}}'
    deps: [check-prereq]
    cmds:
      - echo "Creating VMs..."
      - terraform apply -target=module.vms -auto-approve

  configure:
    desc: "Configure Talos on VMs"
    dir: '{{.TERRAFORM_DIR}}'
    deps: [check-prereq]
    cmds:
      - echo "Waiting for VMs to get IP addresses..."
      - |
        # Extract IPs from terraform output
        VM_IPS=($(terraform output -json | jq -r '.talos_client_configuration.value.nodes[]'))
        {{.ROOT_DIR}}/bootstrap/homelab/wait-vms-ready.sh "${VM_IPS[@]}"
      - terraform apply -var="configure_talos=true" -auto-approve

  kubeconfig:
    desc: "Wait for kubeconfig to be generated by terraform"
    deps: [check-prereq]
    cmds:
      - |
        echo "‚è≥ Waiting for terraform to generate kubeconfig..."
        for i in {1..60}; do
          if [ -f "{{.KUBECONFIG}}" ]; then
            echo "‚úÖ Kubeconfig generated by terraform"
            break
          fi
          echo "  Waiting for kubeconfig file... ($i/60)"
          sleep 5
        done
        if [ ! -f "{{.KUBECONFIG}}" ]; then
          echo "‚ùå Kubeconfig not generated by terraform"
          exit 1
        fi
      - echo "‚è≥ Waiting for Kubernetes API to be ready..."
      - |
        for i in {1..60}; do
          if kubectl get nodes >/dev/null 2>&1; then
            echo "‚úÖ Kubernetes API is ready"
            break
          fi
          echo "  Waiting for API server... ($i/60)"
          sleep 5
        done
      - |
        echo ""
        echo "üìã Initial cluster status (nodes will be NotReady until CNI is installed):"
        kubectl get nodes
        echo ""
        echo "‚úÖ Kubernetes cluster API is ready!"
        echo "‚ÑπÔ∏è  Note: Nodes will remain NotReady until Cilium CNI is installed"

  validate-cluster:
    desc: "Validate complete cluster health (nodes, CNI, DNS)"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    preconditions:
      - sh: kubectl get nodes
        msg: "Cluster must be accessible before validation"
    cmds:
      - |
        echo "üîç Validating complete cluster health..."
        echo ""

        echo "üìã Node Status:"
        kubectl get nodes -o wide
        echo ""

        echo "üåê Cilium Status:"
        kubectl get pods -n kube-system -l k8s-app=cilium
        echo ""

        echo "üîç DNS Status:"
        kubectl get svc -n kube-system kube-dns
        kubectl get endpoints -n kube-system kube-dns
        echo ""

        # Wait for DNS endpoints to be ready
        echo "‚è≥ Waiting for DNS endpoints to be ready..."
        for i in {1..30}; do
          dns_endpoints=$(kubectl get endpoints -n kube-system kube-dns -o jsonpath='{.subsets[0].addresses[*].ip}' 2>/dev/null | wc -w)
          if [ "$dns_endpoints" -gt 0 ]; then
            echo "‚úÖ DNS endpoints are ready ($dns_endpoints endpoints)"
            break
          fi
          echo "  Waiting for DNS endpoints... ($i/30)"
          sleep 2
        done
        echo ""

        # Count ready nodes
        total_nodes=$(kubectl get nodes --no-headers | wc -l | tr -d ' \n\r\t')
        ready_nodes=$(kubectl get nodes --no-headers | grep -w Ready | wc -l | tr -d ' \n\r\t')

        echo "üìä Cluster Health Summary:"
        echo "  Nodes: $ready_nodes/$total_nodes Ready"

        # Check Cilium
        cilium_ready=$(kubectl get pods -n kube-system -l k8s-app=cilium --field-selector=status.phase=Running --no-headers | wc -l | tr -d ' \n\r\t')
        echo "  Cilium: $cilium_ready pods running"

        # Check DNS
        dns_endpoints=$(kubectl get endpoints -n kube-system kube-dns -o jsonpath='{.subsets[0].addresses[*].ip}' 2>/dev/null | wc -w)
        echo "  DNS: $dns_endpoints ready"
        echo ""

        # Final validation
        if [ "$ready_nodes" -gt 0 ] && [ "$cilium_ready" -gt 0 ] && [ "$dns_endpoints" -gt 0 ]; then
          echo "‚úÖ Cluster validation successful!"
          echo "   ‚Ä¢ API server: ‚úÖ Accessible"
          echo "   ‚Ä¢ Nodes: ‚úÖ $ready_nodes ready"
          echo "   ‚Ä¢ CNI: ‚úÖ Cilium running"
          echo "   ‚Ä¢ DNS: ‚úÖ CoreDNS endpoints ready"
          echo ""
          echo "üöÄ Cluster is ready for workloads!"
        else
          echo "‚ö†Ô∏è  Cluster validation issues detected:"
          [ "$ready_nodes" -eq 0 ] && echo "   ‚Ä¢ No nodes are ready"
          [ "$cilium_ready" -eq 0 ] && echo "   ‚Ä¢ Cilium CNI not running"
          [ "$dns_endpoints" -eq 0 ] && echo "   ‚Ä¢ DNS endpoints not ready"
          echo ""
          echo "Run 'task homelab:check-nodes' for detailed troubleshooting"
          exit 1
        fi

  check-nodes:
    desc: "Check node status and provide troubleshooting guidance"
    deps: [check-prereq]
    cmds:
      - |
        echo "üîç Node Status Overview:"
        kubectl get nodes -o wide
        echo ""

        # Check for NotReady nodes
        not_ready_nodes=$(kubectl get nodes --no-headers | grep NotReady | awk '{print $1}' || true)
        if [ -n "$not_ready_nodes" ]; then
          echo "‚ö†Ô∏è  NotReady nodes detected:"
          for node in $not_ready_nodes; do
            echo "  üìã Node: $node"
            echo "     Conditions:"
            kubectl describe node $node | grep -A 10 "Conditions:" | head -15
            echo ""
          done

          echo "üí° Troubleshooting steps:"
          echo "  1. Check if nodes are reachable via ping"
          echo "  2. Restart unresponsive VMs from Proxmox web interface"
          echo "  3. For GPU nodes, check if GPU passthrough is causing issues"
          echo "  4. Run 'kubectl delete node <node-name>' and let it rejoin if needed"
        else
          echo "‚úÖ All nodes are Ready"
        fi

  install-cilium:
    desc: "Install Cilium CNI (required before workers can join)"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    preconditions:
      - sh: kubectl get nodes
        msg: "Cluster must be accessible before installing Cilium"
    cmds:
      - |
        # Extract control plane IP from terraform output and pass to script
        cd {{.ROOT_DIR}}/infrastructure/homelab
        export CONTROL_PLANE_IP=$(terraform output -json | jq -r '.talos_client_configuration.value.endpoints[0]')
        cd {{.ROOT_DIR}}/bootstrap/homelab
        ./install-cilium.sh

  install-fluxcd:
    desc: "Install FluxCD GitOps controller"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    preconditions:
      - sh: kubectl get nodes
        msg: "Cluster must be accessible before installing FluxCD"
    vars:
      FLUX_PATH: '{{.flux_path | default "kubernetes/homelab"}}'
    cmds:
      - |
        # Skip if FluxCD is already installed
        if kubectl get ns flux-system >/dev/null 2>&1; then
          echo "‚úÖ FluxCD is already installed"
          echo "Run 'task homelab:verify' to check infrastructure status"
          exit 0
        fi

        # Install flux CLI if needed
        if ! command -v flux &> /dev/null; then
          echo "üì¶ Installing flux CLI..."
          curl -s https://fluxcd.io/install.sh | sudo bash
        fi

        echo "üöÄ Installing FluxCD..."

        # Check for GitHub token in environment or .env file
        if [ -z "${FLUXCD_GITHUB_TOKEN:-}" ] && [ -f {{.ROOT_DIR}}/.env ]; then
          source {{.ROOT_DIR}}/.env
        fi

        # Validate required environment variables
        if [ -z "${FLUXCD_OWNER:-}" ]; then
          echo "‚ùå FLUXCD_OWNER environment variable is required"
          echo "Set it in .env file or export FLUXCD_OWNER=your-github-username"
          exit 1
        fi

        if [ -z "${FLUXCD_REPOSITORY:-}" ]; then
          echo "‚ùå FLUXCD_REPOSITORY environment variable is required"
          echo "Set it in .env file or export FLUXCD_REPOSITORY=your-repo-name"
          exit 1
        fi

        if [ -n "${FLUXCD_GITHUB_TOKEN:-}" ]; then
          echo "‚úÖ Using GitHub token from environment"
          export GITHUB_TOKEN="$FLUXCD_GITHUB_TOKEN"
        else
          echo "‚ö†Ô∏è  No FLUXCD_GITHUB_TOKEN found - you will be prompted for your GitHub token"
        fi

        echo "üìã FluxCD Configuration:"
        echo "  Owner: $FLUXCD_OWNER"
        echo "  Repository: $FLUXCD_REPOSITORY"
        echo "  Path: {{.FLUX_PATH}}"
        echo "  Branch: main"
        echo "  Kubeconfig: {{.KUBECONFIG}}"
        echo ""

        # Bootstrap FluxCD with explicit kubeconfig
        echo "üîß Using kubeconfig: $KUBECONFIG"
        echo "üîß Testing cluster connectivity..."
        kubectl get nodes || {
          echo "‚ùå Cannot connect to cluster with kubeconfig: $KUBECONFIG"
          exit 1
        }

        echo "üöÄ Running FluxCD bootstrap..."
        flux bootstrap github \
          --owner="$FLUXCD_OWNER" \
          --repository="$FLUXCD_REPOSITORY" \
          --branch=main \
          --path="{{.FLUX_PATH}}" \
          --personal \
          --token-auth || {
          echo "‚ùå FluxCD bootstrap failed"
          exit 1
        }

        echo "‚úÖ FluxCD bootstrap completed, verifying installation..."
        kubectl get pods -n flux-system || {
          echo "‚ùå FluxCD pods not found after bootstrap"
          exit 1
        }

        echo "‚úÖ FluxCD installed and configured successfully!"

  bootstrap:
    desc: "Create cluster and bootstrap FluxCD"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    vars:
      FLUX_PATH: '{{.flux_path | default "kubernetes/homelab"}}'
    cmds:
      - |
        # Create cluster if kubeconfig doesn't exist or can't connect
        if [ ! -f "{{.KUBECONFIG}}" ]; then
          echo "üöÄ Creating Kubernetes cluster..."
          task up
        else
          echo "üîç Checking cluster connectivity..."
          if kubectl get nodes >/dev/null 2>&1; then
            echo "‚úÖ Cluster already exists and is accessible"
          else
            echo "‚ö†Ô∏è  Kubeconfig exists but cluster not accessible, recreating..."
            task up
          fi
        fi

        # Install Cilium CNI if not already installed
        if ! kubectl get pods -n kube-system -l k8s-app=cilium | grep -q Running; then
          echo "üåê Installing Cilium CNI..."
          task install-cilium
        else
          echo "‚úÖ Cilium CNI is already installed"
        fi

        # Install FluxCD (just the installation, no waiting)
        echo "üöÄ Installing FluxCD GitOps..."
        task install-fluxcd flux_path="{{.FLUX_PATH}}"

        # Wait for flux-system namespace to be created
        echo "‚è≥ Waiting for flux-system namespace..."
        for i in {1..30}; do
          if kubectl get namespace flux-system >/dev/null 2>&1; then
            echo "‚úÖ flux-system namespace is ready"
            break
          fi
          echo "  Waiting for flux-system namespace... ($i/30)"
          sleep 2
        done

        # Create required secrets IMMEDIATELY after FluxCD creates namespace
        echo "üîê Creating required secrets before reconciliation starts..."
        task create-vault-transit-secret
        task sync-env-secrets

        # Now wait for the initial sync and controllers to stabilize
        echo "‚è≥ Waiting for FluxCD to sync repository and controllers to be ready..."
        for i in {1..60}; do
          if kubectl get kustomization controllers -n flux-system >/dev/null 2>&1; then
            controllers_status=$(kubectl get kustomization controllers -n flux-system -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")
            if [ "$controllers_status" = "True" ]; then
              echo "‚úÖ Controllers layer is ready"
              break
            fi
          fi
          echo "  Waiting for controllers layer... ($i/60)"
          sleep 5
        done

        echo "‚úÖ Cluster created and FluxCD bootstrapped successfully!"

  verify:
    desc: "Verify all infrastructure components are ready"
    deps: [check-prereq]
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    cmds:
      - echo "Waiting for all infrastructure components..."
      - ./wait-for-infrastructure.sh
      - echo ""
      - echo "‚úÖ All infrastructure ready!"
      - echo ""
      - echo "Platform Foundation status:"
      - kubectl get kustomization -n flux-system | grep -E 'NAME|platform-foundation'
      - echo ""
      - echo "Infrastructure layers status:"
      - kubectl get kustomization -n flux-system | grep -E 'NAME|security|serverless|data-cache|ml-serving|data-storage'
      - echo ""
      - echo "Application status:"
      - kubectl get kustomization -n flux-system | grep -E 'apps|lldap|authelia|openwebui' || echo "No applications deployed yet"

  suspend:
    desc: "Suspend Flux reconciliation (services keep running)"
    deps: [check-prereq]
    cmds:
      - |
        if kubectl get nodes >/dev/null 2>&1; then
          echo "‚è∏Ô∏è  Suspending Flux reconciliation..."

          # Suspend all GitRepositories
          kubectl get gitrepository -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":true}}' || true

          # Suspend all HelmRepositories
          kubectl get helmrepository -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":true}}' || true

          # Suspend all HelmReleases
          kubectl get helmrelease -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":true}}' || true

          # Suspend all Kustomizations
          kubectl get kustomization -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":true}}' || true

          echo "‚úÖ Flux reconciliation suspended"
          echo "‚ÑπÔ∏è  Services continue running but won't be updated"
          echo "‚ÑπÔ∏è  Run 'task homelab:resume' to re-enable reconciliation"
        else
          echo "‚ö†Ô∏è  Cluster not accessible"
        fi

  resume:
    desc: "Resume Flux reconciliation"
    deps: [check-prereq]
    cmds:
      - |
        if kubectl get nodes >/dev/null 2>&1; then
          echo "‚ñ∂Ô∏è  Resuming Flux reconciliation..."

          # Resume all GitRepositories
          kubectl get gitrepository -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":false}}' || true

          # Resume all HelmRepositories
          kubectl get helmrepository -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":false}}' || true

          # Resume all HelmReleases
          kubectl get helmrelease -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":false}}' || true

          # Resume all Kustomizations
          kubectl get kustomization -n flux-system -o name 2>/dev/null | xargs -r -I {} kubectl patch {} -n flux-system --type='merge' -p='{"spec":{"suspend":false}}' || true

          echo "‚úÖ Flux reconciliation resumed"
        else
          echo "‚ö†Ô∏è  Cluster not accessible"
        fi

  destroy:
    desc: "Destroy FluxCD and all deployed resources (inverse of bootstrap)"
    deps: [check-prereq]
    prompt: This will remove FluxCD and all deployed resources. Are you sure?
    cmds:
      - ./destroy-flux.sh
      - echo ""
      - echo "‚úÖ Destroy completed"
      - echo "‚ÑπÔ∏è  Disk cleanup will be handled by Rook's cleanupPolicy during cluster deletion"

  force-cleanup:
    desc: "Force cleanup stuck namespaces (use when destroy doesn't fully clean)"
    deps: [check-prereq]
    cmds:
      - ./force-cleanup-namespaces.sh

  uninstall:
    desc: Uninstall everything (cluster + VMs + configs)
    dir: '{{.TERRAFORM_DIR}}'
    deps: [check-prereq]
    prompt: This will destroy the entire cluster and VMs. Are you sure?
    cmds:
      - echo "üóë Destroying VMs..."
      - terraform destroy -auto-approve
      - echo "üßπ Cleaning up configs..."
      - rm -f ../../kubeconfig ../../talosconfig
      - ../../bootstrap/homelab/cleanup-vms.sh
      - |
        echo
        echo "=== IMPORTANT: GPU Passthrough Users ==="
        echo "If you have VMs with GPU passthrough, you MUST reboot the Proxmox host"
        echo "before creating new VMs with GPU, otherwise you'll get QEMU errors."
        echo
        echo "Run: task homelab:reboot"
        echo
        echo "This will prevent the 'pci_irq_handler: Assertion' error."
      - echo "‚úÖ Uninstall complete"

  cleanup:
    desc: "Clean up existing VMs from Proxmox"
    dir: '{{.TERRAFORM_DIR}}'
    deps: [check-prereq]
    prompt: This will stop and remove all VMs defined in terraform.tfvars. Are you sure?
    cmds:
      - ../../bootstrap/homelab/cleanup-vms.sh

  reboot:
    desc: "Reboot Proxmox host (required after destroying VMs with GPU passthrough)"
    dir: '{{.TERRAFORM_DIR}}'
    prompt: This will reboot the Proxmox host. Are you sure?
    cmds:
      - ../../bootstrap/homelab/reboot-proxmox.sh
